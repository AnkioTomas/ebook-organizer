import json
import os
import re
import time
import random
from xml.sax.saxutils import escape

import PyPDF2
import ebookmeta  # å¤„ç† EPUB/MOBI/AWZ3 æ ¼å¼çš„å…ƒæ•°æ®
import requests
from bs4 import BeautifulSoup
from lxml import etree

# === ç›®å½• & é…ç½® ===
BOOKS_DIR = "./books"  # ä¹¦ç±ç›®å½•
CONFIG_FILE = "./douban_config.json"  # é…ç½®æ–‡ä»¶
NEW_NAME_PATTERN = "{author} - {title} ({year})"  # æ–‡ä»¶å¤¹å‘½åæ ¼å¼
DOUBAN_SEARCH_URL = "https://www.douban.com/search"
# éšæœºUser-Agentåˆ—è¡¨
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:90.0) Gecko/20100101 Firefox/90.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36'
]
# è¯·æ±‚é…ç½®
REQUEST_CONFIG = {
    'timeout': 10,  # è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    'max_retries': 3,  # æœ€å¤§é‡è¯•æ¬¡æ•°
    'retry_delay': [2, 5],  # é‡è¯•å»¶è¿ŸèŒƒå›´ï¼ˆç§’ï¼‰
    'request_delay': [1, 3],  # è¯·æ±‚é—´éš”èŒƒå›´ï¼ˆç§’ï¼‰
    'proxy': None  # ä»£ç†è®¾ç½®ï¼Œæ ¼å¼å¦‚ {'http': 'http://127.0.0.1:7890', 'https': 'http://127.0.0.1:7890'}
}
SUPPORTED_FORMATS = ['pdf', 'epub', 'mobi', 'txt', 'azw3','azw']

# === è¯·æ±‚å·¥å…·å‡½æ•° ===
def get_random_headers():
    """è·å–éšæœºUser-Agentçš„è¯·æ±‚å¤´"""
    return {
        'User-Agent': random.choice(USER_AGENTS),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
        'Referer': 'https://book.douban.com/',
        'Upgrade-Insecure-Requests': '1',
        'Cache-Control': 'max-age=0'
    }

def safe_request(url, method='get', params=None, retry_count=0, **kwargs):
    """å®‰å…¨çš„è¯·æ±‚å‡½æ•°ï¼Œå¸¦æœ‰é‡è¯•ã€å»¶è¿Ÿå’Œå¼‚å¸¸å¤„ç†"""
    # åˆå¹¶é»˜è®¤è¯·æ±‚å¤´å’Œè‡ªå®šä¹‰è¯·æ±‚å¤´
    headers = get_random_headers()
    if 'headers' in kwargs:
        headers.update(kwargs.pop('headers'))
    
    # æ·»åŠ ä»£ç†
    proxies = REQUEST_CONFIG['proxy']
    
    try:
        # è¯·æ±‚å‰éšæœºå»¶è¿Ÿï¼Œé¿å…é¢‘ç¹è¯·æ±‚
        if retry_count > 0 or random.random() < 0.8:  # 80%æ¦‚ç‡å»¶è¿Ÿ
            delay = random.uniform(*REQUEST_CONFIG['request_delay'])
            time.sleep(delay)
        
        # å‘èµ·è¯·æ±‚
        if method.lower() == 'get':
            response = requests.get(
                url, 
                headers=headers, 
                params=params, 
                proxies=proxies,
                timeout=REQUEST_CONFIG['timeout'],
                **kwargs
            )
        else:
            response = requests.post(
                url, 
                headers=headers, 
                data=params, 
                proxies=proxies,
                timeout=REQUEST_CONFIG['timeout'],
                **kwargs
            )
        
        # æ£€æŸ¥å“åº”çŠ¶æ€
        if response.status_code == 200:
            return response
        elif response.status_code == 403 or response.status_code == 429:
            print(f"âš ï¸ è¯·æ±‚è¢«é™åˆ¶ (çŠ¶æ€ç : {response.status_code})ï¼Œç­‰å¾…åé‡è¯•...")
            retry_delay = random.uniform(*REQUEST_CONFIG['retry_delay']) * (retry_count + 1)
            time.sleep(retry_delay)
        else:
            print(f"âš ï¸ è¯·æ±‚å¤±è´¥ (çŠ¶æ€ç : {response.status_code})")
            
    except requests.exceptions.Timeout:
        print("âš ï¸ è¯·æ±‚è¶…æ—¶")
    except requests.exceptions.ConnectionError:
        print("âš ï¸ è¿æ¥é”™è¯¯")
    except Exception as e:
        print(f"âš ï¸ è¯·æ±‚å¼‚å¸¸: {e}")
    
    # é‡è¯•é€»è¾‘
    if retry_count < REQUEST_CONFIG['max_retries']:
        print(f"ğŸ”„ ç¬¬ {retry_count + 1} æ¬¡é‡è¯•...")
        return safe_request(url, method, params, retry_count + 1, **kwargs)
    else:
        print("âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè¯·æ±‚å¤±è´¥")
        return None

# === è§£ææ–‡ä»¶å ===
def parse_filename(filename):
    """
    è§£æç”µå­ä¹¦æ–‡ä»¶åï¼š
    - ä¹¦åï¼šæå–ä¸»è¦æ ‡é¢˜éƒ¨åˆ†
    - ä½œè€…ï¼šä»æ‹¬å·ä¸­æå–å¯èƒ½çš„ä½œè€…
    - å¹´ä»½ï¼šæ‹¬å·å†…çš„å››ä½æ•°å­—
    """
    # ä¿å­˜åŸå§‹æ–‡ä»¶åç”¨äºè°ƒè¯•
    original_name = filename
    name, ext = os.path.splitext(filename)
    ext = ext.lstrip(".")
    
    # ç¬¬ä¸€æ­¥ï¼šæ¸…ç†æ–‡ä»¶åï¼Œåˆ é™¤å¸¸è§çš„æ— å…³æ ‡è®°
    # åˆ é™¤Z-Libraryæ ‡è®°
    name = re.sub(r'\s*\(Z-Library\)', '', name).strip()
    
    # ç¬¬äºŒæ­¥ï¼šæå–å¹´ä»½ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    year = None
    year_match = re.search(r"\((\d{4})\)", name)
    if year_match:
        year = year_match.group(1)
    
    # ç¬¬ä¸‰æ­¥ï¼šæå–ä½œè€…ï¼ˆä¼˜å…ˆä»æ‹¬å·ä¸­æå–ï¼‰
    author = None
    
    # å°è¯•ä»æ‹¬å·ä¸­æå–ä½œè€…
    author_matches = re.findall(r"\(([^()]+)\)", name)
    for possible_author in author_matches:
        possible_author = possible_author.strip()
        # è·³è¿‡å¹´ä»½å’Œæ˜æ˜¾ä¸æ˜¯ä½œè€…çš„å†…å®¹
        if possible_author.isdigit() or len(possible_author) > 20:
            continue
            
        # å¦‚æœåŒ…å«ç‰¹æ®Šæ ‡è®°å¦‚"ã€”æ³•ã€•"ï¼Œå¾ˆå¯èƒ½æ˜¯ä½œè€…
        if re.search(r'ã€”[^ã€•]+ã€•|\([^)]+\)|\[[^\]]+\]|ï¼ˆ[^ï¼‰]+ï¼‰', possible_author):
            # ç›´æ¥å»é™¤å›½ç±æ ‡è®°ï¼Œä¿ç•™ä½œè€…å
            author = re.sub(r'ã€”[^ã€•]+ã€•|\([^)]+\)|\[[^\]]+\]|ï¼ˆ[^ï¼‰]+ï¼‰', '', possible_author).strip()
            break
            
        # å¦åˆ™ï¼Œå¦‚æœé•¿åº¦åˆé€‚ï¼Œå¯èƒ½æ˜¯ä½œè€…
        author = possible_author
        break
    
    # å¦‚æœæ‹¬å·ä¸­æ²¡æ‰¾åˆ°ä½œè€…ï¼Œå°è¯•ä»æ–‡ä»¶åæ ¼å¼æ¨æ–­
    if not author and " - " in name:
        parts = name.split(" - ", 1)
        if len(parts[0].strip()) < 20:  # å¦‚æœå‰åŠéƒ¨åˆ†è¾ƒçŸ­ï¼Œå¯èƒ½æ˜¯ä½œè€…
            author = parts[0].strip()
    
    # ç¬¬å››æ­¥ï¼šæå–æ ‡é¢˜
    title = None
    
    # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰ä¹¦åå·ï¼Œä¼˜å…ˆæå–
    title_match = re.search(r'ã€Š([^ã€‹]+)ã€‹', name)
    if title_match:
        title = title_match.group(1).strip()
    else:
        # å¦‚æœæ²¡æœ‰ä¹¦åå·ï¼Œå°è¯•å»é™¤æ‰€æœ‰æ‹¬å·å†…å®¹åæå–ä¸»è¦éƒ¨åˆ†
        clean_name = re.sub(r'\([^)]*\)|\[[^\]]*\]|ï¼ˆ[^ï¼‰]*ï¼‰|ã€[^ã€‘]*ã€‘', ' ', name).strip()
        
        # å¦‚æœæœ‰æ¨ªçº¿ä¸”å·²ç»æ‰¾åˆ°ä½œè€…ï¼Œå–æ¨ªçº¿åé¢çš„éƒ¨åˆ†ä½œä¸ºæ ‡é¢˜
        if author and " - " in clean_name:
            parts = clean_name.split(" - ", 1)
            if parts[0].strip() == author:
                title = parts[1].strip()
            else:
                title = clean_name
        else:
            title = clean_name
    
    # ç¬¬äº”æ­¥ï¼šæ¸…ç†æ ‡é¢˜å’Œä½œè€…
    if title:
        # åˆ é™¤æ ‡é¢˜ä¸­çš„ç‰¹æ®Šæ ‡è®°
        title = re.sub(r'ã€[^ã€‘]*ã€‘|ã€Š|ã€‹', '', title).strip()
        # å¦‚æœæ ‡é¢˜å¤ªé•¿ï¼Œå°è¯•æˆªå–ä¸»è¦éƒ¨åˆ†
        if len(title) > 30:
            # å°è¯•åœ¨ç¬¬ä¸€ä¸ªæ ‡ç‚¹ç¬¦å·å¤„æˆªæ–­
            short_title_match = re.search(r'^[^ï¼Œã€‚ï¼šï¼›ï¼ï¼Ÿ,.:;!?]+', title)
            if short_title_match:
                title = short_title_match.group(0).strip()
    
    if author:
        # æ¸…ç†ä½œè€…åä¸­çš„ç‰¹æ®Šå­—ç¬¦å’Œå›½ç±æ ‡è®°
        author = re.sub(r'\[[^\]]*\]|ã€”[^ã€•]*ã€•|\([^)]*\)|ï¼ˆ[^ï¼‰]*ï¼‰|ã€[^ã€‘]*ã€‘', '', author).strip()
    
    # æœ€åä¸€æ­¥ï¼šç¡®ä¿æ ‡é¢˜å’Œä½œè€…æ²¡æœ‰å‰å¯¼å’Œå°¾éƒ¨çš„ç‰¹æ®Šå­—ç¬¦
    if title:
        title = re.sub(r'^[^\u4e00-\u9fa5a-zA-Z0-9]+|[^\u4e00-\u9fa5a-zA-Z0-9]+$', '', title).strip()
    if author:
        author = re.sub(r'^[^\u4e00-\u9fa5a-zA-Z0-9]+|[^\u4e00-\u9fa5a-zA-Z0-9]+$', '', author).strip()
    
    # è°ƒè¯•ä¿¡æ¯
    # print(f"è§£æ '{original_name}' => ä½œè€…: '{author}', æ ‡é¢˜: '{title}', å¹´ä»½: '{year}'")
    
    return author, title, year, ext


# === è§£æ PDF æ–‡ä»¶å…ƒæ•°æ® ===
def extract_pdf_metadata(pdf_path):
    """å°è¯•ä» PDF æ–‡ä»¶å…ƒæ•°æ®ä¸­æå–ä¹¦ç±ä¿¡æ¯"""
    try:
        with open(pdf_path, "rb") as f:
            reader = PyPDF2.PdfReader(f)
            metadata = reader.metadata
            title = metadata.get("/Title", "").strip() if metadata else None
            author = metadata.get("/Author", "").strip() if metadata else None
            return author, title
    except Exception as e:
        print(f"âš ï¸ æ— æ³•è¯»å– PDF å…ƒæ•°æ®: {e}")
        return None, None


# === è§£æ EPUB/MOBI/AWZ3 æ–‡ä»¶å…ƒæ•°æ® ===
def extract_ebook_metadata(file_path):
    """å°è¯•ä» EPUB/MOBI/AWZ3 æ–‡ä»¶çš„å…ƒæ•°æ®ä¸­æå–ä¹¦ç±ä¿¡æ¯"""
    try:
        metadata = ebookmeta.get_metadata(file_path)
        return metadata.get("author"), metadata.get("title")
    except Exception as e:
        print(f"âš ï¸ æ— æ³•è¯»å–ç”µå­ä¹¦å…ƒæ•°æ®: {e}")
        return None, None


# === ä»è±†ç“£è·å–ä¹¦ç±ä¿¡æ¯ ===
def search_douban(query, expected_author=None, fetch_detail=True):
    """åœ¨è±†ç“£æœç´¢ä¹¦ç±ï¼Œè¿”å›ç¬¬ä¸€ä¸ªåŒ¹é…çš„ä¹¦ç±ä¿¡æ¯
    Args:
        query: æœç´¢å…³é”®è¯
        expected_author: å·²åºŸå¼ƒå‚æ•°ï¼Œä¿ç•™æ˜¯ä¸ºäº†å…¼å®¹æ€§
        fetch_detail: æ˜¯å¦è·å–è¯¦æƒ…é¡µä¿¡æ¯ï¼ˆå¯é€‰ï¼Œé»˜è®¤Trueï¼‰
    """
    params = {"cat": "1001", "q": query}
    res = safe_request(DOUBAN_SEARCH_URL, params=params)
    
    if not res:
        print(f"âŒ è±†ç“£æœç´¢å¤±è´¥")
        return None

    soup = BeautifulSoup(res.content, 'html.parser')
    results = soup.select('.result-list .result')
    
    for result in results:
        try:
            # è·å–æ ‡é¢˜å’Œé“¾æ¥
            title_elem = result.select_one('.title h3 a')
            if not title_elem:
                continue
            
            title = title_elem.get_text(strip=True).replace(' ', '')
            book_url = title_elem.get('href', '')
            
            # ä»é‡å®šå‘URLä¸­æå–çœŸå®çš„è±†ç“£å›¾ä¹¦é“¾æ¥
            subject_id = None
            if 'link2' in book_url:
                from urllib.parse import parse_qs, urlparse
                parsed = urlparse(book_url)
                query_params = parse_qs(parsed.query)
                real_url = query_params.get('url', [''])[0]
                
                # URLè§£ç 
                real_url = requests.utils.unquote(real_url)
                
                # ä»çœŸå®URLä¸­æå–ID
                subject_match = re.search(r'subject/(\d+)', real_url)
                if subject_match:
                    subject_id = subject_match.group(1)
            
            if not subject_id:
                continue
                
            # æ„å»ºçœŸå®çš„è±†ç“£å›¾ä¹¦URL
            real_book_url = f'https://book.douban.com/subject/{subject_id}/'
            
            # è·å–è¯„åˆ†ä¿¡æ¯
            rating_info = result.select_one('.rating-info')
            if not rating_info:
                continue
                
            # è·å–å‡ºç‰ˆä¿¡æ¯
            subject_info = rating_info.select_one('.subject-cast').get_text(strip=True)
            parts = subject_info.split('/')
            parts = [p.strip() for p in parts]
            
            # è§£æä½œè€…ã€å‡ºç‰ˆç¤¾ã€å¹´ä»½
            author = parts[0]
            publisher = parts[-2] if len(parts) > 2 else None
            
            # å°è¯•ä»å‡ºç‰ˆä¿¡æ¯ä¸­æå–å¹´ä»½
            year = None
            for part in parts:
                if part.strip().isdigit() and len(part.strip()) == 4:
                    year = part.strip()
                    break
            
            # è·å–å°é¢å›¾ç‰‡URL
            cover_elem = result.select_one('.pic img')
            cover_url = cover_elem.get('src') if cover_elem else None
            
            # è·å–è¯„åˆ†å’Œè¯„ä»·äººæ•°
            rating = rating_info.select_one('.rating_nums')
            rating = rating.get_text(strip=True) if rating else None
            
            rating_people = rating_info.select_one('.rating_nums + span')
            rating_people = rating_people.get_text(strip=True).strip('(äººè¯„ä»·)') if rating_people else None
            
            # è·å–ç®€ä»‹
            intro = result.select_one('.content p')
            intro = intro.get_text(strip=True) if intro else None

            book_info = {
                "title": title,
                "author": author,
                "year": year,
                "publisher": publisher,
                "cover_url": cover_url,
                "rating": rating,
                "rating_people": rating_people,
                "intro": intro,
                "url": real_book_url,
                "douban_id": subject_id
            }

            # è·å–è¯¦æƒ…é¡µä¿¡æ¯ï¼ˆISBNç­‰ï¼‰
            if fetch_detail and real_book_url:
                detail_info = fetch_douban_book_info(real_book_url)
                if detail_info:
                    book_info.update(detail_info)
            
            return book_info

        except Exception as e:
            print(f"è§£ææœç´¢ç»“æœæ—¶å‡ºé”™: {e}")
            continue
            
    return None


# === è§£æè±†ç“£ä¹¦ç±è¯¦æƒ…é¡µ ===
def fetch_douban_book_info(book_url):
    """è§£æè±†ç“£ä¹¦ç±è¯¦æƒ…é¡µï¼Œè¿”å›è¡¥å……ä¿¡æ¯"""
    try:
        res = safe_request(book_url)
        if not res:
            return None

        soup = BeautifulSoup(res.content, 'html.parser')
        
        # è·å–å›¾ä¹¦ä¿¡æ¯åŒºåŸŸ
        info = soup.select_one('#info')
        if not info:
            return None
            
        # è·å–æ‰€æœ‰æ–‡æœ¬è¡Œ
        info_text = info.get_text()

        # è§£æè¯¦ç»†ä¿¡æ¯
        def extract_field(field):
            pattern = f'{field}:\\s*([^\\n]+)'
            match = re.search(pattern, info_text)
            return match.group(1).strip() if match else None
            
        # è·å–å„ç§ä¿¡æ¯
        isbn = extract_field( 'ISBN')
        pages = extract_field( 'é¡µæ•°')
        price = extract_field( 'å®šä»·')
        binding = extract_field( 'è£…å¸§')
        series = extract_field( 'ä¸›ä¹¦')
        publish_year = extract_field( 'å‡ºç‰ˆå¹´')
        publisher = extract_field( 'å‡ºç‰ˆç¤¾')
        
        # è·å–ä½œè€…ï¼ˆå¯èƒ½æœ‰å¤šä¸ªï¼‰
        authors = []
        author_links = info.select('a[href^="/author/"]')
        if author_links:
            authors = [a.get_text(strip=True) for a in author_links]
            
        # è·å–è¯‘è€…ï¼ˆå¦‚æœæœ‰ï¼‰
        translators = []
        translator_text = extract_field( 'è¯‘è€…')
        if translator_text:
            translators = [t.strip() for t in translator_text.split(',')]

        # è·å–æ ‡ç­¾
        tags = []
        # æŸ¥æ‰¾åŒ…å«criteriaçš„scriptæ ‡ç­¾
        script_tags = soup.find_all('script', type='text/javascript')
        for script in script_tags:
            script_text = script.string
            if script_text and 'criteria' in script_text:
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–criteriaä¸­çš„æ ‡ç­¾
                criteria_match = re.search(r"criteria\s*=\s*'([^']*)'", script_text)
                if criteria_match:
                    criteria_text = criteria_match.group(1)
                    # åˆ†å‰²å¹¶æå–7:å¼€å¤´çš„æ ‡ç­¾ï¼Œæ’é™¤åŒ…å«subjectçš„æ ‡ç­¾
                    tag_parts = [part for part in criteria_text.split('|') 
                               if part.startswith('7:') and 'subject' not in part]
                    tags = [part.split(':')[1] for part in tag_parts]
                break

        # å¦‚æœä»JSä¸­æ²¡æœ‰æ‰¾åˆ°æ ‡ç­¾ï¼Œå°è¯•ä»é¡µé¢ä¸­æå–
        if not tags:
            tag_elements = soup.select('a.tag')
            if tag_elements:
                tags = [tag.get_text(strip=True) for tag in tag_elements]

        # è·å–å®Œæ•´ç®€ä»‹
        full_intro = None
        intro_element = soup.select_one('#link-report .intro')
        if intro_element:
            full_intro = intro_element.get_text(strip=True)
        else:
            # å°è¯•å…¶ä»–å¯èƒ½çš„ç®€ä»‹ä½ç½®
            intro_element = soup.select_one('.related_info .intro')
            if intro_element:
                full_intro = intro_element.get_text(strip=True)

            
        # è·å–è¯„åˆ†ä¿¡æ¯
        rating = None
        rating_element = soup.select_one('.rating_self strong.rating_num')
        if rating_element:
            rating = rating_element.get_text(strip=True)
            
        rating_people = None
        people_element = soup.select_one('.rating_sum .rating_people')
        if people_element:
            rating_people = people_element.get_text(strip=True).replace('äººè¯„ä»·', '')

        return {
            "isbn": isbn,
            "pages": pages,
            "price": price,
            "binding": binding,
            "series": series,
            "publish_year": publish_year,
            "publisher": publisher,
            "authors": authors,
            "translators": translators,
            "tags": tags,
            "full_intro": full_intro,

            "rating": rating,
            "rating_people": rating_people
        }

    except Exception as e:
        print(f"è·å–è¯¦æƒ…é¡µä¿¡æ¯æ—¶å‡ºé”™: {e}")
        return None


# === ä¸‹è½½å°é¢ ===
def download_cover(url, save_path):
    """ä¸‹è½½è±†ç“£å°é¢"""
    try:
        res = safe_request(url)
        if not res:
            print("âŒ ä¸‹è½½å°é¢å¤±è´¥")
            return
            
        with open(save_path, "wb") as f:
            f.write(res.content)
    except Exception as e:
        print(f"âŒ ä¸‹è½½å°é¢å¤±è´¥: {e}")


# === ç”Ÿæˆ NFO æ–‡ä»¶ ===
def generate_nfo(book_info, save_path):
    """åˆ›å»º NFO æ–‡ä»¶ï¼ŒXMLæ ¼å¼"""
    if not book_info:
        return
        
    def safe_xml(text):
        """ç¡®ä¿æ–‡æœ¬å®‰å…¨ç”¨äºXML"""
        if not text:
            return ""
        return escape(str(text))
        
    # æ„å»ºXMLå†…å®¹
    xml_content = '<?xml version="1.0" encoding="UTF-8"?>\n'
    xml_content += '<book>\n'
    
    # æ·»åŠ åŸºæœ¬ä¿¡æ¯
    xml_content += f'    <title>{safe_xml(book_info.get("title", ""))}</title>\n'
    xml_content += f'    <publish_date>{safe_xml(book_info.get("publish_year", ""))}</publish_date>\n'
    xml_content += f'    <year>{safe_xml(book_info.get("year", ""))}</year>\n'
    xml_content += f'    <isbn>{safe_xml(book_info.get("isbn", ""))}</isbn>\n'
    xml_content += f'    <language>ä¸­æ–‡</language>\n'
    
    # æ·»åŠ æ ‡ç­¾
    if book_info.get("tags"):
        # å°†æ‰€æœ‰æ ‡ç­¾ç”¨/è¿æ¥æˆä¸€ä¸ªå­—ç¬¦ä¸²
        tags_str = " / ".join(book_info["tags"])
        xml_content += f'    <tag>{safe_xml(tags_str)}</tag>\n'
            
    # æ·»åŠ genreï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªæ ‡ç­¾ä½œä¸ºgenreï¼‰
    if book_info.get("tags"):
        xml_content += f'    <genre>{safe_xml(book_info["tags"][0])}</genre>\n'
    else:
        xml_content += '    <genre></genre>\n'
    
    # æ·»åŠ å…¶ä»–ä¿¡æ¯
    xml_content += f'    <publisher>{safe_xml(book_info.get("publisher", ""))}</publisher>\n'
    
    # è·å–ä½œè€…ä¿¡æ¯ï¼Œä¼˜å…ˆä½¿ç”¨authorså­—æ®µ
    artist = ""
    if book_info.get("authors") and len(book_info["authors"]) > 0:
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªä½œè€…
        artist = book_info["authors"][0]
    else:
        artist = book_info.get("author", "")
        
    # æ¸…ç†ä½œè€…åä¸­çš„å›½ç±æ ‡è®°
    if artist:
        artist = re.sub(r'[\[ï¼ˆ\(ã€ã€”][^\]ï¼‰\)ã€‘ã€•]*[\]ï¼‰\)ã€‘ã€•]', '', artist).strip()
        
    xml_content += f'    <artist>{safe_xml(artist)}</artist>\n'
    
    # æ·»åŠ ç®€ä»‹ï¼Œä¼˜å…ˆä½¿ç”¨å®Œæ•´ç®€ä»‹
    intro = book_info.get("full_intro") or book_info.get("intro") or ""
    xml_content += f'    <introduction>{safe_xml(intro)}</introduction>\n'
    
    xml_content += '</book>'
    
    # å†™å…¥æ–‡ä»¶
    with open(save_path, "w", encoding="utf-8") as f:
        f.write(xml_content)


# === æ–‡ä»¶é‡å‘½åä¸»é€»è¾‘ ===
def rename_books():
    """éå†ç›®å½•ï¼Œé‡å‘½åä¹¦ç±æ–‡ä»¶ï¼Œå¹¶æ•´ç†åˆ°ç‹¬ç«‹æ–‡ä»¶å¤¹"""
    # ç¡®ä¿booksç›®å½•å­˜åœ¨
    if not os.path.exists(BOOKS_DIR):
        os.makedirs(BOOKS_DIR)
        print(f"âœ… å·²åˆ›å»ºä¹¦ç±ç›®å½•: {BOOKS_DIR}")
        return  # å¦‚æœæ˜¯æ–°åˆ›å»ºçš„ç›®å½•ï¼Œé‡Œé¢æ²¡æœ‰æ–‡ä»¶ï¼Œç›´æ¥è¿”å›
        
    for filename in os.listdir(BOOKS_DIR):
        file_path = os.path.join(BOOKS_DIR, filename)
        if not os.path.isfile(file_path):
            continue

        author, title, year, ext = parse_filename(filename)

        if not title or not author:
            if ext.lower() == "pdf":
                meta_author, meta_title = extract_pdf_metadata(file_path)
                if meta_author: author = meta_author
                if meta_title: title = meta_title
            elif ext.lower() in ["epub", "mobi", "azw3","azw"]:
                meta_author, meta_title = extract_ebook_metadata(file_path)
                if meta_author: author = meta_author
                if meta_title: title = meta_title
        douban_info = None
        # æ— è®ºæ˜¯å¦æœ‰æ ‡é¢˜æˆ–ä½œè€…ï¼Œéƒ½å°è¯•ä»è±†ç“£è·å–ä¿¡æ¯
        if title:
            douban_info = search_douban(title)
            if douban_info:
                # ä¼˜å…ˆä½¿ç”¨è±†ç“£çš„ä½œè€…ä¿¡æ¯
                title = douban_info["title"]
                year = douban_info.get("year")
                
                # ä¼˜å…ˆä½¿ç”¨è±†ç“£è¯¦æƒ…é¡µçš„authorså­—æ®µ
                if douban_info.get("authors") and len(douban_info["authors"]) > 0:
                    author = douban_info["authors"][0]
                else:
                    author = douban_info["author"]
                
                # æ¸…ç†ä½œè€…åä¸­çš„å›½ç±æ ‡è®°
                if author:
                    # ç§»é™¤å¦‚ã€”æ³•ã€•ã€ï¼ˆç¾ï¼‰ç­‰å›½ç±æ ‡è®°
                    author = re.sub(r'[\[ï¼ˆ\(ã€ã€”][^\]ï¼‰\)ã€‘ã€•]*[\]ï¼‰\)ã€‘ã€•]', '', author).strip()
                    # ç§»é™¤å¯èƒ½çš„å‰å¯¼å’Œå°¾éƒ¨ç‰¹æ®Šå­—ç¬¦
                    author = re.sub(r'^[^\u4e00-\u9fa5a-zA-Z0-9]+|[^\u4e00-\u9fa5a-zA-Z0-9]+$', '', author).strip()

        # å¦‚æœæ ‡é¢˜æˆ–ä½œè€…ä¸ºç©ºï¼Œè¯·æ±‚ç”¨æˆ·è¾“å…¥
        if not title:
            title = input("âš ï¸ æœªèƒ½è·å–åˆ°ä¹¦ç±æ ‡é¢˜ï¼Œè¯·æ‰‹åŠ¨è¾“å…¥: ").strip()
        if not author:
            author = input("âš ï¸ æœªèƒ½è·å–åˆ°ä½œè€…ä¿¡æ¯ï¼Œè¯·æ‰‹åŠ¨è¾“å…¥: ").strip()
            
        # ç¡®ä¿å¿…è¦ä¿¡æ¯å­˜åœ¨
        if not title or not author:
            print(f"âŒ æ— æ³•å¤„ç†æ–‡ä»¶ {filename}ï¼šç¼ºå°‘å¿…è¦çš„æ ‡é¢˜æˆ–ä½œè€…ä¿¡æ¯")
            continue

        # æ ¹æ®æ˜¯å¦æœ‰å¹´ä»½ä¿¡æ¯ä½¿ç”¨ä¸åŒçš„å‘½åæ¨¡å¼
        if year:
            folder_name = NEW_NAME_PATTERN.format(author=author, title=title, year=year)
        else:
            folder_name = f"{author} - {title}"
            
        folder_path = os.path.join(BOOKS_DIR, folder_name)
        new_book_path = os.path.join(folder_path, f"{title}.{ext}")

        # æ˜¾ç¤ºå°†è¦æ‰§è¡Œçš„æ“ä½œå¹¶ç­‰å¾…ç”¨æˆ·ç¡®è®¤
        print("\n=== å³å°†æ‰§è¡Œä»¥ä¸‹æ“ä½œ ===")
        print(f"åŸæ–‡ä»¶: {filename}")
        print(f"æ–°æ–‡ä»¶å¤¹: {folder_name}")
        print(f"æ–°æ–‡ä»¶å: {title}.{ext}")
        if douban_info and douban_info.get("cover_url"):
            print("å°†ä¸‹è½½è±†ç“£å°é¢")
        print("å°†ç”ŸæˆNFOæ–‡ä»¶")
        
        confirm = input("\næ˜¯å¦ç»§ç»­ï¼Ÿ(è¾“å…¥ 'no' å–æ¶ˆï¼Œå…¶ä»–ä»»æ„é”®ç»§ç»­): ").strip().lower()
        if confirm == 'no':
            print("å·²å–æ¶ˆæ“ä½œ")
            continue

        # æ‰§è¡Œæ–‡ä»¶æ“ä½œ
        os.makedirs(folder_path, exist_ok=True)
        os.rename(file_path, new_book_path)

        if douban_info and douban_info.get("cover_url"):
            cover_path = os.path.join(folder_path, f"{title}.jpg")
            download_cover(douban_info["cover_url"], cover_path)

        nfo_path = os.path.join(folder_path, f"{title}.nfo")
        generate_nfo(douban_info, nfo_path)
        print(f"âœ… æ–‡ä»¶å¤„ç†å®Œæˆ: {title}\n")


# === é…ç½®ç®¡ç† ===
def save_config():
    """ä¿å­˜å½“å‰é…ç½®åˆ°æ–‡ä»¶"""
    config = {
        'proxy': REQUEST_CONFIG['proxy'],
        'request_delay': REQUEST_CONFIG['request_delay'],
        'retry_delay': REQUEST_CONFIG['retry_delay'],
        'timeout': REQUEST_CONFIG['timeout'],
        'max_retries': REQUEST_CONFIG['max_retries']
    }
    try:
        with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        print(f"âœ… é…ç½®å·²ä¿å­˜åˆ° {CONFIG_FILE}")
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜é…ç½®å¤±è´¥: {e}")

def load_config():
    """ä»æ–‡ä»¶åŠ è½½é…ç½®"""
    if not os.path.exists(CONFIG_FILE):
        return False
        
    try:
        with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
            config = json.load(f)
            
        # æ›´æ–°é…ç½®
        for key, value in config.items():
            if key in REQUEST_CONFIG:
                REQUEST_CONFIG[key] = value
                
        print(f"âœ… å·²åŠ è½½é…ç½®: {CONFIG_FILE}")
        
        # æ˜¾ç¤ºå½“å‰é…ç½®
        if REQUEST_CONFIG['proxy']:
            proxy_info = REQUEST_CONFIG['proxy'].get('http', 'None')
            print(f"  - ä»£ç†: {proxy_info}")
        print(f"  - è¯·æ±‚å»¶è¿Ÿ: {REQUEST_CONFIG['request_delay'][0]}-{REQUEST_CONFIG['request_delay'][1]}ç§’")
        print(f"  - è¶…æ—¶: {REQUEST_CONFIG['timeout']}ç§’")
        print(f"  - æœ€å¤§é‡è¯•: {REQUEST_CONFIG['max_retries']}æ¬¡")
        
        return True
    except Exception as e:
        print(f"âš ï¸ åŠ è½½é…ç½®å¤±è´¥: {e}")
        return False


if __name__ == "__main__":
    print("=" * 50)
    print("ğŸ“š ç”µå­ä¹¦æ–‡ä»¶æ•´ç†å·¥å…·")
    print("=" * 50)
    print("åŠŸèƒ½ï¼šè‡ªåŠ¨è§£æç”µå­ä¹¦æ–‡ä»¶åï¼Œæ•´ç†åˆ°ç‹¬ç«‹æ–‡ä»¶å¤¹ï¼Œå¹¶è·å–è±†ç“£ä¿¡æ¯")
    print(f"ä¹¦ç±ç›®å½•: {BOOKS_DIR}")
    print("æ”¯æŒæ ¼å¼: " + ", ".join(SUPPORTED_FORMATS))
    print("=" * 50)
    
    # åŠ è½½é…ç½®
    has_config = load_config()
    
    # é…ç½®ç½‘ç»œè®¾ç½®
    print("\n=== ç½‘ç»œè®¾ç½® ===")
    if has_config:
        use_saved = input("æ£€æµ‹åˆ°å·²ä¿å­˜çš„é…ç½®ï¼Œæ˜¯å¦ä½¿ç”¨? (y/n, é»˜è®¤: y): ").strip().lower() != 'n'
        if not use_saved:
            has_config = False
    
    if not has_config:
        print("(å¯é€‰) é…ç½®ä»£ç†å’Œè¯·æ±‚å‚æ•°ï¼Œé¿å…IPå°é”")
        
        use_proxy = input("æ˜¯å¦ä½¿ç”¨ä»£ç†? (y/n, é»˜è®¤: n): ").strip().lower() == 'y'
        if use_proxy:
            proxy_host = input("è¯·è¾“å…¥ä»£ç†åœ°å€ (ä¾‹å¦‚: 127.0.0.1): ").strip()
            proxy_port = input("è¯·è¾“å…¥ä»£ç†ç«¯å£ (ä¾‹å¦‚: 7890): ").strip()
            if proxy_host and proxy_port:
                proxy_url = f"http://{proxy_host}:{proxy_port}"
                REQUEST_CONFIG['proxy'] = {
                    'http': proxy_url,
                    'https': proxy_url
                }
                print(f"âœ… å·²è®¾ç½®ä»£ç†: {proxy_url}")
        
        # é…ç½®è¯·æ±‚å»¶è¿Ÿ
        custom_delay = input("æ˜¯å¦è‡ªå®šä¹‰è¯·æ±‚å»¶è¿Ÿ? (y/n, é»˜è®¤: n): ").strip().lower() == 'y'
        if custom_delay:
            try:
                min_delay = float(input("æœ€å°å»¶è¿Ÿç§’æ•° (é»˜è®¤: 1): ").strip() or "1")
                max_delay = float(input("æœ€å¤§å»¶è¿Ÿç§’æ•° (é»˜è®¤: 3): ").strip() or "3")
                if min_delay > 0 and max_delay >= min_delay:
                    REQUEST_CONFIG['request_delay'] = [min_delay, max_delay]
                    print(f"âœ… å·²è®¾ç½®è¯·æ±‚å»¶è¿Ÿ: {min_delay}-{max_delay}ç§’")
            except ValueError:
                print("âš ï¸ è¾“å…¥æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å»¶è¿Ÿè®¾ç½®")
        
        # ä¿å­˜é…ç½®
        save_config_choice = input("æ˜¯å¦ä¿å­˜å½“å‰é…ç½®? (y/n, é»˜è®¤: y): ").strip().lower() != 'n'
        if save_config_choice:
            save_config()
    
    print("\nå¼€å§‹å¤„ç†ä¹¦ç±æ–‡ä»¶...")
    rename_books()
    
    print("\nâœ… å¤„ç†å®Œæˆï¼")
    print("=" * 50)
