import difflib
import json
import os
import random
import re
import time
from xml.sax.saxutils import escape

import PyPDF2
import colorama
import ebookmeta  # å¤„ç† EPUB/MOBI/AWZ3 æ ¼å¼çš„å…ƒæ•°æ®
import requests
from bs4 import BeautifulSoup
import opencc  # ç”¨äºç¹ä½“è½¬ç®€ä½“

# åˆå§‹åŒ–ç¹ä½“è½¬ç®€ä½“è½¬æ¢å™¨
converter = opencc.OpenCC('t2s')

# ç¹ä½“è½¬ç®€ä½“å‡½æ•°
def to_simplified(text):
    """å°†ç¹ä½“å­—è½¬æ¢ä¸ºç®€ä½“å­—"""
    if not text:
        return text
    return converter.convert(text)

# åˆå§‹åŒ–coloramaï¼Œæ”¯æŒWindowsä¸‹çš„å½©è‰²è¾“å‡º
colorama.init(autoreset=True)

# === æ—¥å¿—é…ç½® ===
# å®šä¹‰å½©è‰²æ‰“å°å‡½æ•°ï¼Œæ›¿ä»£logger
def print_debug(msg):
    """æ‰“å°è°ƒè¯•ä¿¡æ¯ï¼ˆé’è‰²ï¼‰"""
    print(f"{colorama.Fore.CYAN}DEBUG: {msg}{colorama.Style.RESET_ALL}")

def print_info(msg):
    """æ‰“å°ä¿¡æ¯ï¼ˆç»¿è‰²ï¼‰"""
    print(f"{colorama.Fore.GREEN}INFO: {msg}{colorama.Style.RESET_ALL}")

def print_warning(msg):
    """æ‰“å°è­¦å‘Šï¼ˆé»„è‰²ï¼‰"""
    print(f"{colorama.Fore.YELLOW}WARNING: {msg}{colorama.Style.RESET_ALL}")

def print_error(msg):
    """æ‰“å°é”™è¯¯ï¼ˆçº¢è‰²ï¼‰"""
    print(f"{colorama.Fore.RED}ERROR: {msg}{colorama.Style.RESET_ALL}")

def print_critical(msg):
    """æ‰“å°ä¸¥é‡é”™è¯¯ï¼ˆçº¢åº•ç™½å­—ï¼‰"""
    print(f"{colorama.Back.RED}{colorama.Fore.WHITE}CRITICAL: {msg}{colorama.Style.RESET_ALL}")

# å­—ç¬¦ç”»å’Œä½œè€…ä¿¡æ¯
ASCII_ART = r"""
 _____             _                    ______             _      _____                      _              
|  __ \           | |                  |  ____|           | |    / ____|                    (_)             
| |  | | ___  _   | |__   __ _ _ __    | |__   ___   ___ | | __| |     ___  _ ____   _____ _ _ __ ___ _ __ 
| |  | |/ _ \| | | | '_ \ / _` | '_ \   |  __| / _ \ / _ \| |/ /| |    / _ \| '_ \ \ / / _ \ | '__/ _ \ '__|
| |__| | (_) | |_| | |_) | (_| | | | |  | |___| (_) | (_) |   < | |___| (_) | | | \ V /  __/ | | |  __/ |   
|_____/ \___/ \__,_|_.__/ \__,_|_| |_|  |______\___/ \___/|_|\_\ \_____\___/|_| |_|\_/ \___|_|_|  \___|_|   
"""

AUTHOR_INFO = """
ä½œè€…: Ankio
ç‰ˆæœ¬: 1.0.0
æ—¥æœŸ: 2024-01-07
æè¿°: ç”µå­ä¹¦æ–‡ä»¶æ•´ç†å·¥å…·ï¼Œè‡ªåŠ¨è§£ææ–‡ä»¶åï¼Œè·å–è±†ç“£ä¿¡æ¯ï¼Œæ•´ç†åˆ°ç‹¬ç«‹æ–‡ä»¶å¤¹
"""

# === ç›®å½• & é…ç½® ===
BOOKS_DIR = "./books"  # ä¹¦ç±ç›®å½•
CONFIG_FILE = "./douban_config.json"  # é…ç½®æ–‡ä»¶
NEW_NAME_PATTERN = "{author} - {title} ({year})"  # æ–‡ä»¶å¤¹å‘½åæ ¼å¼
DOUBAN_SEARCH_URL = "https://www.douban.com/search"
# éšæœºUser-Agentåˆ—è¡¨
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:90.0) Gecko/20100101 Firefox/90.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36'
]
# è¯·æ±‚é…ç½®
REQUEST_CONFIG = {
    'timeout': 10,  # è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    'max_retries': 3,  # æœ€å¤§é‡è¯•æ¬¡æ•°
    'retry_delay': [2, 5],  # é‡è¯•å»¶è¿ŸèŒƒå›´ï¼ˆç§’ï¼‰
    'request_delay': [1, 3],  # è¯·æ±‚é—´éš”èŒƒå›´ï¼ˆç§’ï¼‰
    'proxy': None  # ä»£ç†è®¾ç½®ï¼Œæ ¼å¼å¦‚ {'http': 'http://127.0.0.1:7890', 'https': 'http://127.0.0.1:7890'}
}
SUPPORTED_FORMATS = ['pdf', 'epub', 'mobi', 'txt', 'azw3','azw']

# === è¯·æ±‚å·¥å…·å‡½æ•° ===
def get_random_headers():
    """è·å–éšæœºUser-Agentçš„è¯·æ±‚å¤´"""
    return {
        'User-Agent': random.choice(USER_AGENTS),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
        'Referer': 'https://book.douban.com/',
        'Upgrade-Insecure-Requests': '1',
        'Cache-Control': 'max-age=0'
    }

def safe_request(url, method='get', params=None, retry_count=0, **kwargs):
    """å®‰å…¨çš„è¯·æ±‚å‡½æ•°ï¼Œå¸¦æœ‰é‡è¯•ã€å»¶è¿Ÿå’Œå¼‚å¸¸å¤„ç†"""
    # åˆå¹¶é»˜è®¤è¯·æ±‚å¤´å’Œè‡ªå®šä¹‰è¯·æ±‚å¤´
    headers = get_random_headers()
    if 'headers' in kwargs:
        headers.update(kwargs.pop('headers'))
    
    # æ·»åŠ ä»£ç†
    proxies = REQUEST_CONFIG['proxy']
    
    print_debug(f"å‘èµ·è¯·æ±‚: {method.upper()} {url}")
    if params:
        print_debug(f"è¯·æ±‚å‚æ•°: {params}")
    if proxies:
        print_debug(f"ä½¿ç”¨ä»£ç†: {proxies}")
    
    try:
        # è¯·æ±‚å‰éšæœºå»¶è¿Ÿï¼Œé¿å…é¢‘ç¹è¯·æ±‚
        if retry_count > 0 or random.random() < 0.8:  # 80%æ¦‚ç‡å»¶è¿Ÿ
            delay = random.uniform(*REQUEST_CONFIG['request_delay'])
            print_debug(f"è¯·æ±‚å»¶è¿Ÿ: {delay:.2f}ç§’")
            time.sleep(delay)
        
        # å‘èµ·è¯·æ±‚
        if method.lower() == 'get':
            response = requests.get(
                url, 
                headers=headers, 
                params=params, 
                proxies=proxies,
                timeout=REQUEST_CONFIG['timeout'],
                **kwargs
            )
        else:
            response = requests.post(
                url, 
                headers=headers, 
                data=params, 
                proxies=proxies,
                timeout=REQUEST_CONFIG['timeout'],
                **kwargs
            )
        
        # æ£€æŸ¥å“åº”çŠ¶æ€
        if response.status_code == 200:
            print_info(f"è¯·æ±‚æˆåŠŸ: {response.status_code}")
            return response
        elif response.status_code == 403 or response.status_code == 429:
            print_warning(f"è¯·æ±‚è¢«é™åˆ¶ (çŠ¶æ€ç : {response.status_code})ï¼Œç­‰å¾…åé‡è¯•...")
            retry_delay = random.uniform(*REQUEST_CONFIG['retry_delay']) * (retry_count + 1)
            print_debug(f"é‡è¯•å»¶è¿Ÿ: {retry_delay:.2f}ç§’")
            time.sleep(retry_delay)
        else:
            print_warning(f"è¯·æ±‚å¤±è´¥ (çŠ¶æ€ç : {response.status_code})")
            
    except requests.exceptions.Timeout:
        print_error("è¯·æ±‚è¶…æ—¶")
    except requests.exceptions.ConnectionError:
        print_error("è¿æ¥é”™è¯¯")
    except Exception as e:
        print_error(f"è¯·æ±‚å¼‚å¸¸: {e}")
    
    # é‡è¯•é€»è¾‘
    if retry_count < REQUEST_CONFIG['max_retries']:
        print_info(f"ç¬¬ {retry_count + 1} æ¬¡é‡è¯•...")
        return safe_request(url, method, params, retry_count + 1, **kwargs)
    else:
        print_error("è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè¯·æ±‚å¤±è´¥")
        return None

# === è§£ææ–‡ä»¶å ===
def parse_filename(filename):
    """
    è§£æç”µå­ä¹¦æ–‡ä»¶åï¼š
    - ä¹¦åï¼šæå–ä¸»è¦æ ‡é¢˜éƒ¨åˆ†
    - ä½œè€…ï¼šä»æ‹¬å·ä¸­æå–å¯èƒ½çš„ä½œè€…
    - å¹´ä»½ï¼šæ‹¬å·å†…çš„å››ä½æ•°å­—
    """
    # ä¿å­˜åŸå§‹æ–‡ä»¶åç”¨äºè°ƒè¯•
    original_name = filename
    name, ext = os.path.splitext(filename)
    ext = ext.lstrip(".")
    
    # ç¬¬ä¸€æ­¥ï¼šæ¸…ç†æ–‡ä»¶åï¼Œåˆ é™¤å¸¸è§çš„æ— å…³æ ‡è®°
    # åˆ é™¤Z-Libraryæ ‡è®°
    name = re.sub(r'\s*\(Z-Library\)', '', name).strip()
    
    # ç¬¬äºŒæ­¥ï¼šæå–å¹´ä»½ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    year = None
    year_match = re.search(r"\((\d{4})\)", name)
    if year_match:
        year = year_match.group(1)
    
    # ç¬¬ä¸‰æ­¥ï¼šæå–ä½œè€…ï¼ˆä¼˜å…ˆä»æ‹¬å·ä¸­æå–ï¼‰
    author = None
    
    # å°è¯•ä»æ‹¬å·ä¸­æå–ä½œè€…
    author_matches = re.findall(r"\(([^()]+)\)", name)
    for possible_author in author_matches:
        possible_author = possible_author.strip()
        # è·³è¿‡å¹´ä»½å’Œæ˜æ˜¾ä¸æ˜¯ä½œè€…çš„å†…å®¹
        if possible_author.isdigit() or len(possible_author) > 20:
            continue
            
        # å¦‚æœåŒ…å«ç‰¹æ®Šæ ‡è®°å¦‚"ã€”æ³•ã€•"ï¼Œå¾ˆå¯èƒ½æ˜¯ä½œè€…
        if re.search(r'ã€”[^ã€•]+ã€•|\([^)]+\)|\[[^\]]+\]|ï¼ˆ[^ï¼‰]+ï¼‰', possible_author):
            # ç›´æ¥å»é™¤å›½ç±æ ‡è®°ï¼Œä¿ç•™ä½œè€…å
            author = re.sub(r'ã€”[^ã€•]+ã€•|\([^)]+\)|\[[^\]]+\]|ï¼ˆ[^ï¼‰]+ï¼‰', '', possible_author).strip()
            break
            
        # å¦åˆ™ï¼Œå¦‚æœé•¿åº¦åˆé€‚ï¼Œå¯èƒ½æ˜¯ä½œè€…
        author = possible_author
        break
    
    # å¦‚æœæ‹¬å·ä¸­æ²¡æ‰¾åˆ°ä½œè€…ï¼Œå°è¯•ä»æ–‡ä»¶åæ ¼å¼æ¨æ–­
    if not author and " - " in name:
        parts = name.split(" - ", 1)
        if len(parts[0].strip()) < 20:  # å¦‚æœå‰åŠéƒ¨åˆ†è¾ƒçŸ­ï¼Œå¯èƒ½æ˜¯ä½œè€…
            author = parts[0].strip()
    
    # ç¬¬å››æ­¥ï¼šæå–æ ‡é¢˜
    title = None
    
    # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰ä¹¦åå·ï¼Œä¼˜å…ˆæå–
    title_match = re.search(r'ã€Š([^ã€‹]+)ã€‹', name)
    if title_match:
        title = title_match.group(1).strip()
    else:
        # å¦‚æœæ²¡æœ‰ä¹¦åå·ï¼Œå°è¯•å»é™¤æ‰€æœ‰æ‹¬å·å†…å®¹åæå–ä¸»è¦éƒ¨åˆ†
        clean_name = re.sub(r'\([^)]*\)|\[[^\]]*\]|ï¼ˆ[^ï¼‰]*ï¼‰|ã€[^ã€‘]*ã€‘', ' ', name).strip()
        
        # å¦‚æœæœ‰æ¨ªçº¿ä¸”å·²ç»æ‰¾åˆ°ä½œè€…ï¼Œå–æ¨ªçº¿åé¢çš„éƒ¨åˆ†ä½œä¸ºæ ‡é¢˜
        if author and " - " in clean_name:
            parts = clean_name.split(" - ", 1)
            if parts[0].strip() == author:
                title = parts[1].strip()
            else:
                title = clean_name
        else:
            title = clean_name
    
    # ç¬¬äº”æ­¥ï¼šæ¸…ç†æ ‡é¢˜å’Œä½œè€…
    if title:
        # åˆ é™¤æ ‡é¢˜ä¸­çš„ç‰¹æ®Šæ ‡è®°
        title = re.sub(r'ã€[^ã€‘]*ã€‘|ã€Š|ã€‹', '', title).strip()
        # å¦‚æœæ ‡é¢˜å¤ªé•¿ï¼Œå°è¯•æˆªå–ä¸»è¦éƒ¨åˆ†
        if len(title) > 30:
            # å°è¯•åœ¨ç¬¬ä¸€ä¸ªæ ‡ç‚¹ç¬¦å·å¤„æˆªæ–­
            short_title_match = re.search(r'^[^ï¼Œã€‚ï¼šï¼›ï¼ï¼Ÿ,.:;!?]+', title)
            if short_title_match:
                title = short_title_match.group(0).strip()
    
    if author:
        # æ¸…ç†ä½œè€…åä¸­çš„ç‰¹æ®Šå­—ç¬¦å’Œå›½ç±æ ‡è®°
        author = re.sub(r'\[[^\]]*\]|ã€”[^ã€•]*ã€•|\([^)]*\)|ï¼ˆ[^ï¼‰]*ï¼‰|ã€[^ã€‘]*ã€‘', '', author).strip()
    
    # æœ€åä¸€æ­¥ï¼šç¡®ä¿æ ‡é¢˜å’Œä½œè€…æ²¡æœ‰å‰å¯¼å’Œå°¾éƒ¨çš„ç‰¹æ®Šå­—ç¬¦
    if title:
        title = re.sub(r'^[^\u4e00-\u9fa5a-zA-Z0-9]+|[^\u4e00-\u9fa5a-zA-Z0-9]+$', '', title).strip()
        # è½¬æ¢ä¸ºç®€ä½“å­—
        title = to_simplified(title)
    if author:
        author = re.sub(r'^[^\u4e00-\u9fa5a-zA-Z0-9]+|[^\u4e00-\u9fa5a-zA-Z0-9]+$', '', author).strip()
        # è½¬æ¢ä¸ºç®€ä½“å­—
        author = to_simplified(author)
    
    # è°ƒè¯•ä¿¡æ¯
    # print(f"è§£æ '{original_name}' => ä½œè€…: '{author}', æ ‡é¢˜: '{title}', å¹´ä»½: '{year}'")
    
    return author, title, year, ext


# === è§£æ PDF æ–‡ä»¶å…ƒæ•°æ® ===
def extract_pdf_metadata(pdf_path):
    """å°è¯•ä» PDF æ–‡ä»¶å…ƒæ•°æ®ä¸­æå–ä¹¦ç±ä¿¡æ¯"""
    try:
        with open(pdf_path, "rb") as f:
            reader = PyPDF2.PdfReader(f)
            metadata = reader.metadata
            title = metadata.get("/Title", "").strip() if metadata else None
            author = metadata.get("/Author", "").strip() if metadata else None
            # è½¬æ¢ä¸ºç®€ä½“å­—
            if title:
                title = to_simplified(title)
            if author:
                author = to_simplified(author)
            return author, title
    except Exception as e:
        print_error(f"æ— æ³•è¯»å– PDF å…ƒæ•°æ®: {e}")
        return None, None


# === è§£æ EPUB/MOBI/AWZ3 æ–‡ä»¶å…ƒæ•°æ® ===
def extract_ebook_metadata(file_path):
    """å°è¯•ä» EPUB/MOBI/AWZ3 æ–‡ä»¶çš„å…ƒæ•°æ®ä¸­æå–ä¹¦ç±ä¿¡æ¯"""
    try:
        metadata = ebookmeta.get_metadata(file_path)
        author = metadata.get("author")
        title = metadata.get("title")
        # è½¬æ¢ä¸ºç®€ä½“å­—
        if title:
            title = to_simplified(title)
        if author:
            author = to_simplified(author)
        return author, title
    except Exception as e:
        print_error(f"æ— æ³•è¯»å–ç”µå­ä¹¦å…ƒæ•°æ®: {e}")
        return None, None


# === ä»è±†ç“£è·å–ä¹¦ç±ä¿¡æ¯ ===
def calculate_title_similarity(title1, title2):
    """è®¡ç®—ä¸¤ä¸ªæ ‡é¢˜çš„ç›¸ä¼¼åº¦"""
    # ç§»é™¤ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·ï¼Œä¾¿äºæ¯”è¾ƒ
    def normalize(text):
        # å…ˆè½¬ä¸ºç®€ä½“å­—
        text = to_simplified(text)
        return re.sub(r'[\s.,ï¼Œã€‚:ï¼š;ï¼›!ï¼?ï¼Ÿã€Šã€‹\[\]ã€ã€‘()ï¼ˆï¼‰]', '', text.lower())
    
    norm_title1 = normalize(title1)
    norm_title2 = normalize(title2)
    
    # ä½¿ç”¨difflibè®¡ç®—ç›¸ä¼¼åº¦
    similarity = difflib.SequenceMatcher(None, norm_title1, norm_title2).ratio()
    
    # å¦‚æœä¸€ä¸ªæ ‡é¢˜æ˜¯å¦ä¸€ä¸ªçš„å­ä¸²ï¼Œå¢åŠ ç›¸ä¼¼åº¦
    if norm_title1 in norm_title2 or norm_title2 in norm_title1:
        similarity = max(similarity, 0.8)  # è‡³å°‘80%ç›¸ä¼¼
        
    return similarity

def search_douban(query, expected_author=None, fetch_detail=True, min_similarity=0.6):
    """åœ¨è±†ç“£æœç´¢ä¹¦ç±ï¼Œè¿”å›åŒ¹é…åº¦æœ€é«˜çš„ä¹¦ç±ä¿¡æ¯
    Args:
        query: æœç´¢å…³é”®è¯
        expected_author: é¢„æœŸçš„ä½œè€…åï¼ˆå¯é€‰ï¼Œç”¨äºæ¯”è¾ƒï¼‰
        fetch_detail: æ˜¯å¦è·å–è¯¦æƒ…é¡µä¿¡æ¯ï¼ˆå¯é€‰ï¼Œé»˜è®¤Trueï¼‰
        min_similarity: æœ€å°æ ‡é¢˜ç›¸ä¼¼åº¦ï¼ˆå¯é€‰ï¼Œé»˜è®¤0.6ï¼‰
    """
    print_info(f"æœç´¢è±†ç“£: '{query}'")
    params = {"cat": "1001", "q": query}
    res = safe_request(DOUBAN_SEARCH_URL, params=params)
    
    if not res:
        print_error("è±†ç“£æœç´¢å¤±è´¥")
        return None

    soup = BeautifulSoup(res.content, 'html.parser')
    results = soup.select('.result-list .result')
    
    print_info(f"æ‰¾åˆ° {len(results)} ä¸ªæœç´¢ç»“æœ")
    
    # å­˜å‚¨æ‰€æœ‰åŒ¹é…çš„ç»“æœï¼ŒæŒ‰ç›¸ä¼¼åº¦æ’åº
    matched_results = []
    
    for index, result in enumerate(results):
        try:
            # è·å–æ ‡é¢˜å’Œé“¾æ¥
            title_elem = result.select_one('.title h3 a')
            if not title_elem:
                print_debug(f"ç»“æœ #{index+1}: æ— æ³•æ‰¾åˆ°æ ‡é¢˜å…ƒç´ ")
                continue
            
            # è·å–æ ‡é¢˜å¹¶è½¬ä¸ºç®€ä½“å­—
            title = to_simplified(title_elem.get_text(strip=True).replace(' ', ''))
            book_url = title_elem.get('href', '')
            
            # è®¡ç®—æ ‡é¢˜ç›¸ä¼¼åº¦
            similarity = calculate_title_similarity(query, title)
            print_info(f"ç»“æœ #{index+1}: æ ‡é¢˜='{title}', ç›¸ä¼¼åº¦={similarity:.2f}")
            
            # å¦‚æœç›¸ä¼¼åº¦å¤ªä½ï¼Œè·³è¿‡
            if similarity < min_similarity:
                print_debug(f"ç»“æœ #{index+1}: æ ‡é¢˜ç›¸ä¼¼åº¦è¿‡ä½ ({similarity:.2f} < {min_similarity})")
                continue
            
            # ä»é‡å®šå‘URLä¸­æå–çœŸå®çš„è±†ç“£å›¾ä¹¦é“¾æ¥
            subject_id = None
            if 'link2' in book_url:
                from urllib.parse import parse_qs, urlparse
                parsed = urlparse(book_url)
                query_params = parse_qs(parsed.query)
                real_url = query_params.get('url', [''])[0]
                
                # URLè§£ç 
                real_url = requests.utils.unquote(real_url)
                
                # ä»çœŸå®URLä¸­æå–ID
                subject_match = re.search(r'subject/(\d+)', real_url)
                if subject_match:
                    subject_id = subject_match.group(1)
            
            if not subject_id:
                print_debug(f"ç»“æœ #{index+1}: æ— æ³•æå–è±†ç“£ID")
                continue
                
            # æ„å»ºçœŸå®çš„è±†ç“£å›¾ä¹¦URL
            real_book_url = f'https://book.douban.com/subject/{subject_id}/'
            print_debug(f"ç»“æœ #{index+1}: è±†ç“£URL={real_book_url}")
            
            # è·å–è¯„åˆ†ä¿¡æ¯
            rating_info = result.select_one('.rating-info')
            if not rating_info:
                print_debug(f"ç»“æœ #{index+1}: æ— æ³•æ‰¾åˆ°è¯„åˆ†ä¿¡æ¯")
                continue
                
            # è·å–å‡ºç‰ˆä¿¡æ¯
            subject_cast = rating_info.select_one('.subject-cast')
            if not subject_cast:
                print_debug(f"ç»“æœ #{index+1}: æ— æ³•æ‰¾åˆ°å‡ºç‰ˆä¿¡æ¯")
                continue
                
            subject_info = to_simplified(subject_cast.get_text(strip=True))
            parts = subject_info.split('/')
            parts = [p.strip() for p in parts]
            
            # è§£æä½œè€…ã€å‡ºç‰ˆç¤¾ã€å¹´ä»½
            author = to_simplified(parts[0])
            publisher = to_simplified(parts[-2]) if len(parts) > 2 else None
            
            # å°è¯•ä»å‡ºç‰ˆä¿¡æ¯ä¸­æå–å¹´ä»½
            year = None
            for part in parts:
                if part.strip().isdigit() and len(part.strip()) == 4:
                    year = part.strip()
                    break
            
            print_debug(f"ç»“æœ #{index+1}: ä½œè€…='{author}', å‡ºç‰ˆç¤¾='{publisher}', å¹´ä»½='{year}'")
            
            # è·å–å°é¢å›¾ç‰‡URL
            cover_elem = result.select_one('.pic img')
            cover_url = None
            if cover_elem:
                cover_url = cover_elem.get('src')
            
            # è·å–è¯„åˆ†å’Œè¯„ä»·äººæ•°
            rating = rating_info.select_one('.rating_nums')
            rating = rating.get_text(strip=True) if rating else None
            
            rating_people = rating_info.select_one('.rating_nums + span')
            rating_people = rating_people.get_text(strip=True).strip('(äººè¯„ä»·)') if rating_people else None
            
            print_debug(f"ç»“æœ #{index+1}: è¯„åˆ†={rating}, è¯„ä»·äººæ•°={rating_people}")
            
            # è·å–ç®€ä»‹
            intro = result.select_one('.content p')
            intro = to_simplified(intro.get_text(strip=True)) if intro else None

            book_info = {
                "title": title,
                "author": author,
                "year": year,
                "publisher": publisher,
                "cover_url": cover_url,
                "rating": rating,
                "rating_people": rating_people,
                "intro": intro,
                "url": real_book_url,
                "douban_id": subject_id,
                "similarity": similarity,
                "index": index + 1  # ä¿å­˜ç»“æœçš„åºå·ï¼Œç”¨äºç”¨æˆ·é€‰æ‹©
            }
            
            # å°†ç»“æœæ·»åŠ åˆ°åŒ¹é…åˆ—è¡¨
            matched_results.append(book_info)

        except Exception as e:
            print_error(f"è§£ææœç´¢ç»“æœ #{index+1} æ—¶å‡ºé”™: {e}")
            continue
    
    # å¦‚æœæ²¡æœ‰åŒ¹é…çš„ç»“æœï¼Œè¿”å›None
    if not matched_results:
        print_warning("æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„ä¹¦ç±")
        return None
        
    # æŒ‰ç›¸ä¼¼åº¦æ’åº
    matched_results.sort(key=lambda x: x["similarity"], reverse=True)
    
    # è·å–æœ€é«˜ç›¸ä¼¼åº¦
    highest_similarity = matched_results[0]["similarity"]
    
    # ç­›é€‰å‡ºç›¸ä¼¼åº¦ç›¸åŒçš„æœ€é«˜åŒ¹é…ç»“æœ
    top_matches = [r for r in matched_results if abs(r["similarity"] - highest_similarity) < 0.01]
    
    print_info(f"æ‰¾åˆ° {len(top_matches)} ä¸ªæœ€ä½³åŒ¹é…ç»“æœ (ç›¸ä¼¼åº¦: {highest_similarity:.2f})")
    
    # å¦‚æœåªæœ‰ä¸€ä¸ªæœ€ä½³åŒ¹é…ï¼Œç›´æ¥è¿”å›
    if len(top_matches) == 1:
        best_match = top_matches[0]
        print_info(f"æœ€ä½³åŒ¹é…: '{best_match['title']}' (ç›¸ä¼¼åº¦: {best_match['similarity']:.2f})")
    else:
        # å¦‚æœæœ‰å¤šä¸ªåŒ¹é…ä¸”æä¾›äº†é¢„æœŸä½œè€…ï¼Œå°è¯•åŒ¹é…ä½œè€…
        if expected_author and len(top_matches) > 1:
            # æ¸…ç†é¢„æœŸä½œè€…å
            expected_author = re.sub(r'[\[ï¼ˆ\(ã€ã€”][^\]ï¼‰\)ã€‘ã€•]*[\]ï¼‰\)ã€‘ã€•]', '', expected_author).strip()
            expected_author = to_simplified(expected_author)
            
            # å°è¯•æ‰¾åˆ°ä½œè€…åŒ¹é…çš„ç»“æœ
            author_matches = []
            for match in top_matches:
                # æ¸…ç†ä½œè€…åä»¥ä¾¿æ¯”è¾ƒ
                clean_author = re.sub(r'[\[ï¼ˆ\(ã€ã€”][^\]ï¼‰\)ã€‘ã€•]*[\]ï¼‰\)ã€‘ã€•]', '', match["author"]).strip()
                
                # è®¡ç®—ä½œè€…ç›¸ä¼¼åº¦
                author_similarity = difflib.SequenceMatcher(None, expected_author.lower(), clean_author.lower()).ratio()
                match["author_similarity"] = author_similarity
                
                # å¦‚æœä½œè€…ç›¸ä¼¼åº¦é«˜ï¼ŒåŠ å…¥åŒ¹é…åˆ—è¡¨
                if author_similarity > 0.7:
                    author_matches.append(match)
            
            # å¦‚æœæ‰¾åˆ°ä½œè€…åŒ¹é…çš„ç»“æœï¼ŒæŒ‰ä½œè€…ç›¸ä¼¼åº¦æ’åº
            if author_matches:
                author_matches.sort(key=lambda x: x["author_similarity"], reverse=True)
                best_match = author_matches[0]
                print_info(f"æ ¹æ®ä½œè€…åŒ¹é…é€‰æ‹©: '{best_match['title']}' ä½œè€…: '{best_match['author']}' (ä½œè€…ç›¸ä¼¼åº¦: {best_match['author_similarity']:.2f})")
            else:
                # å¦‚æœæ²¡æœ‰ä½œè€…åŒ¹é…ï¼Œè®©ç”¨æˆ·é€‰æ‹©
                best_match = user_select_match(top_matches)
        else:
            # å¦‚æœæ²¡æœ‰é¢„æœŸä½œè€…æˆ–åªæœ‰ä¸€ä¸ªåŒ¹é…ï¼Œè®©ç”¨æˆ·é€‰æ‹©
            best_match = user_select_match(top_matches)
    
    # è·å–è¯¦æƒ…é¡µä¿¡æ¯ï¼ˆISBNç­‰ï¼‰
    if fetch_detail and best_match["url"]:
        print_info(f"è·å–è¯¦æƒ…é¡µä¿¡æ¯: {best_match['url']}")
        detail_info = fetch_douban_book_info(best_match["url"])
        if detail_info:
            best_match.update(detail_info)
    
    return best_match

def user_select_match(matches):
    """è®©ç”¨æˆ·ä»å¤šä¸ªåŒ¹é…ç»“æœä¸­é€‰æ‹©ä¸€ä¸ª"""
    print("\n" + colorama.Fore.CYAN + "=== æ‰¾åˆ°å¤šä¸ªåŒ¹é…ç»“æœï¼Œè¯·é€‰æ‹© ===" + colorama.Style.RESET_ALL)
    
    for i, match in enumerate(matches):
        print(f"{colorama.Fore.YELLOW}[{i+1}] {colorama.Fore.GREEN}{match['title']}{colorama.Style.RESET_ALL}")
        print(f"   ä½œè€…: {colorama.Fore.WHITE}{match['author']}{colorama.Style.RESET_ALL}")
        print(f"   å‡ºç‰ˆç¤¾: {colorama.Fore.WHITE}{match['publisher'] or 'æœªçŸ¥'}{colorama.Style.RESET_ALL}")
        print(f"   å¹´ä»½: {colorama.Fore.WHITE}{match['year'] or 'æœªçŸ¥'}{colorama.Style.RESET_ALL}")
        print(f"   è¯„åˆ†: {colorama.Fore.WHITE}{match['rating'] or 'æœªçŸ¥'} ({match['rating_people'] or '0'}äººè¯„ä»·){colorama.Style.RESET_ALL}")
        if match.get('intro'):
            # æˆªå–ç®€ä»‹çš„å‰50ä¸ªå­—ç¬¦
            short_intro = match['intro'][:50] + ('...' if len(match['intro']) > 50 else '')
            print(f"   ç®€ä»‹: {colorama.Fore.WHITE}{short_intro}{colorama.Style.RESET_ALL}")
        print()
    
    # è·å–ç”¨æˆ·é€‰æ‹©
    while True:
        try:
            choice = input(f"{colorama.Fore.YELLOW}è¯·è¾“å…¥é€‰æ‹©çš„åºå· (1-{len(matches)}): {colorama.Style.RESET_ALL}")
            choice_idx = int(choice) - 1
            if 0 <= choice_idx < len(matches):
                selected = matches[choice_idx]
                print_info(f"å·²é€‰æ‹©: '{selected['title']}' ä½œè€…: '{selected['author']}'")
                return selected
            else:
                print_error(f"æ— æ•ˆçš„é€‰æ‹©ï¼Œè¯·è¾“å…¥1-{len(matches)}ä¹‹é—´çš„æ•°å­—")
        except ValueError:
            print_error("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")


# === è§£æè±†ç“£ä¹¦ç±è¯¦æƒ…é¡µ ===
def fetch_douban_book_info(book_url):
    """è§£æè±†ç“£ä¹¦ç±è¯¦æƒ…é¡µï¼Œè¿”å›è¡¥å……ä¿¡æ¯"""
    print_info(f"è·å–ä¹¦ç±è¯¦æƒ…: {book_url}")
    try:
        res = safe_request(book_url)
        if not res:
            print_error("è·å–è¯¦æƒ…é¡µå¤±è´¥")
            return None

        soup = BeautifulSoup(res.content, 'html.parser')
        
        # è·å–å›¾ä¹¦ä¿¡æ¯åŒºåŸŸ
        info = soup.select_one('#info')
        if not info:
            print_error("æ— æ³•æ‰¾åˆ°å›¾ä¹¦ä¿¡æ¯åŒºåŸŸ")
            return None
            
        # è·å–æ‰€æœ‰æ–‡æœ¬è¡Œ
        info_text = info.get_text()

        # è§£æè¯¦ç»†ä¿¡æ¯
        def extract_field(field):
            pattern = f'{field}:\\s*([^\\n]+)'
            match = re.search(pattern, info_text)
            value = match.group(1).strip() if match else None
            # è½¬æ¢ä¸ºç®€ä½“å­—
            if value:
                value = to_simplified(value)
            print_debug(f"æå–å­—æ®µ '{field}': {value}")
            return value
            
        # è·å–å„ç§ä¿¡æ¯
        isbn = extract_field('ISBN')
        pages = extract_field('é¡µæ•°')
        price = extract_field('å®šä»·')
        binding = extract_field('è£…å¸§')
        series = extract_field('ä¸›ä¹¦')
        publish_year = extract_field('å‡ºç‰ˆå¹´')
        publisher = extract_field('å‡ºç‰ˆç¤¾')
        
        # è·å–ä½œè€…ï¼ˆå¯èƒ½æœ‰å¤šä¸ªï¼‰
        authors = []
        author_links = info.select('a[href^="/author/"]')
        if author_links:
            authors = [to_simplified(a.get_text(strip=True)) for a in author_links]
            print_debug(f"æ‰¾åˆ°ä½œè€…: {authors}")
            
        # è·å–è¯‘è€…ï¼ˆå¦‚æœæœ‰ï¼‰
        translators = []
        translator_text = extract_field('è¯‘è€…')
        if translator_text:
            translators = [to_simplified(t.strip()) for t in translator_text.split(',')]
            print_debug(f"æ‰¾åˆ°è¯‘è€…: {translators}")

        # è·å–æ ‡ç­¾
        tags = []
        # æŸ¥æ‰¾åŒ…å«criteriaçš„scriptæ ‡ç­¾
        script_tags = soup.find_all('script', type='text/javascript')
        for script in script_tags:
            script_text = script.string
            if script_text and 'criteria' in script_text:
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–criteriaä¸­çš„æ ‡ç­¾
                criteria_match = re.search(r"criteria\s*=\s*'([^']*)'", script_text)
                if criteria_match:
                    criteria_text = criteria_match.group(1)
                    # åˆ†å‰²å¹¶æå–7:å¼€å¤´çš„æ ‡ç­¾ï¼Œæ’é™¤åŒ…å«subjectçš„æ ‡ç­¾
                    tag_parts = [part for part in criteria_text.split('|') 
                               if part.startswith('7:') and 'subject' not in part]
                    tags = [to_simplified(part.split(':')[1]) for part in tag_parts]
                break

        # å¦‚æœä»JSä¸­æ²¡æœ‰æ‰¾åˆ°æ ‡ç­¾ï¼Œå°è¯•ä»é¡µé¢ä¸­æå–
        if not tags:
            tag_elements = soup.select('a.tag')
            if tag_elements:
                tags = [to_simplified(tag.get_text(strip=True)) for tag in tag_elements]
        
        # æ ‡ç­¾å»é‡
        tags = list(dict.fromkeys(tags))  # ä¿æŒåŸæœ‰é¡ºåºçš„å»é‡æ–¹æ³•
        
        print_debug(f"æ‰¾åˆ°æ ‡ç­¾: {tags}")

        # è·å–å®Œæ•´ç®€ä»‹
        full_intro = None
        intro_element = soup.select_one('#link-report .intro')
        if intro_element:
            full_intro = to_simplified(intro_element.get_text(strip=True))
        else:
            # å°è¯•å…¶ä»–å¯èƒ½çš„ç®€ä»‹ä½ç½®
            intro_element = soup.select_one('.related_info .intro')
            if intro_element:
                full_intro = to_simplified(intro_element.get_text(strip=True))
        
        if full_intro:
            print_debug(f"æ‰¾åˆ°å®Œæ•´ç®€ä»‹: {full_intro[:50]}...")

            
        # è·å–è¯„åˆ†ä¿¡æ¯
        rating = None
        rating_element = soup.select_one('.rating_self strong.rating_num')
        if rating_element:
            rating = rating_element.get_text(strip=True)
            
        rating_people = None
        people_element = soup.select_one('.rating_sum .rating_people')
        if people_element:
            rating_people = people_element.get_text(strip=True).replace('äººè¯„ä»·', '')
        
        print_debug(f"è¯„åˆ†: {rating}, è¯„ä»·äººæ•°: {rating_people}")

        detail_info = {
            "isbn": isbn,
            "pages": pages,
            "price": price,
            "binding": binding,
            "series": series,
            "publish_year": publish_year,
            "publisher": publisher,
            "authors": authors,
            "translators": translators,
            "tags": tags,
            "full_intro": full_intro,
            "rating": rating,
            "rating_people": rating_people
        }
        
        print_info(f"æˆåŠŸè·å–ä¹¦ç±è¯¦æƒ…: ISBN={isbn}, å‡ºç‰ˆç¤¾={publisher}, å‡ºç‰ˆå¹´={publish_year}")
        return detail_info

    except Exception as e:
        print_error(f"è·å–è¯¦æƒ…é¡µä¿¡æ¯æ—¶å‡ºé”™: {e}")
        return None


# === ä¸‹è½½å°é¢ ===
def download_cover(url, save_path):
    """ä¸‹è½½è±†ç“£å°é¢"""
    print_info(f"ä¸‹è½½å°é¢: {url}")
    try:
        res = safe_request(url)
        if not res:
            print_error("ä¸‹è½½å°é¢å¤±è´¥")
            return
            
        with open(save_path, "wb") as f:
            f.write(res.content)
        print_info(f"å°é¢å·²ä¿å­˜åˆ°: {save_path}")
    except Exception as e:
        print_error(f"ä¸‹è½½å°é¢å¤±è´¥: {e}")


# === ç”Ÿæˆ NFO æ–‡ä»¶ ===
def generate_nfo(book_info, save_path):
    """åˆ›å»º NFO æ–‡ä»¶ï¼ŒXMLæ ¼å¼"""
    if not book_info:
        print_warning("æ²¡æœ‰ä¹¦ç±ä¿¡æ¯ï¼Œæ— æ³•ç”ŸæˆNFOæ–‡ä»¶")
        return
    
    print_info(f"ç”ŸæˆNFOæ–‡ä»¶: {save_path}")
        
    def safe_xml(text):
        """ç¡®ä¿æ–‡æœ¬å®‰å…¨ç”¨äºXML"""
        if not text:
            return ""
        # ç¡®ä¿æ˜¯ç®€ä½“å­—
        text = to_simplified(str(text))
        return escape(text)
        
    # æ„å»ºXMLå†…å®¹
    xml_content = '<?xml version="1.0" encoding="UTF-8"?>\n'
    xml_content += '<book>\n'
    
    # æ·»åŠ åŸºæœ¬ä¿¡æ¯
    xml_content += f'    <title>{safe_xml(book_info.get("title", ""))}</title>\n'
    xml_content += f'    <publish_date>{safe_xml(book_info.get("publish_year", ""))}</publish_date>\n'
    xml_content += f'    <year>{safe_xml(book_info.get("year", ""))}</year>\n'
    xml_content += f'    <isbn>{safe_xml(book_info.get("isbn", ""))}</isbn>\n'
    xml_content += f'    <language>ä¸­æ–‡</language>\n'
    
    # æ·»åŠ æ ‡ç­¾
    if book_info.get("tags"):
        # å°†æ‰€æœ‰æ ‡ç­¾ç”¨/è¿æ¥æˆä¸€ä¸ªå­—ç¬¦ä¸²
        tags_str = " / ".join(book_info["tags"])
        xml_content += f'    <tag>{safe_xml(tags_str)}</tag>\n'
            
    # æ·»åŠ genreï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªæ ‡ç­¾ä½œä¸ºgenreï¼‰
    if book_info.get("tags"):
        xml_content += f'    <genre>{safe_xml(book_info["tags"][0])}</genre>\n'
    else:
        xml_content += '    <genre></genre>\n'
    
    # æ·»åŠ å…¶ä»–ä¿¡æ¯
    xml_content += f'    <publisher>{safe_xml(book_info.get("publisher", ""))}</publisher>\n'
    
    # è·å–ä½œè€…ä¿¡æ¯ï¼Œä¼˜å…ˆä½¿ç”¨authorså­—æ®µ
    artist = ""
    if book_info.get("authors") and len(book_info["authors"]) > 0:
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªä½œè€…
        artist = book_info["authors"][0]
        print_debug(f"ä½¿ç”¨è¯¦æƒ…é¡µä½œè€…: {artist}")
    else:
        artist = book_info.get("author", "")
        print_debug(f"ä½¿ç”¨æœç´¢ç»“æœä½œè€…: {artist}")
        
    # æ¸…ç†ä½œè€…åä¸­çš„å›½ç±æ ‡è®°
    if artist:
        original_artist = artist
        artist = re.sub(r'[\[ï¼ˆ\(ã€ã€”][^\]ï¼‰\)ã€‘ã€•]*[\]ï¼‰\)ã€‘ã€•]', '', artist).strip()
        if artist != original_artist:
            print_debug(f"æ¸…ç†ä½œè€…åä¸­çš„å›½ç±æ ‡è®°: '{original_artist}' -> '{artist}'")
        
    xml_content += f'    <artist>{safe_xml(artist)}</artist>\n'
    
    # æ·»åŠ ç®€ä»‹ï¼Œä¼˜å…ˆä½¿ç”¨å®Œæ•´ç®€ä»‹
    intro = book_info.get("full_intro") or book_info.get("intro") or ""
    if intro:
        print_debug(f"ä½¿ç”¨ç®€ä»‹: {intro[:50]}...")
    xml_content += f'    <introduction>{safe_xml(intro)}</introduction>\n'
    
    xml_content += '</book>'
    
    # å†™å…¥æ–‡ä»¶
    try:
        with open(save_path, "w", encoding="utf-8") as f:
            f.write(xml_content)
        print_info(f"NFOæ–‡ä»¶å·²ä¿å­˜: {save_path}")
    except Exception as e:
        print_error(f"ä¿å­˜NFOæ–‡ä»¶å¤±è´¥: {e}")


# === æ–‡ä»¶é‡å‘½åä¸»é€»è¾‘ ===
def rename_books():
    """éå†ç›®å½•ï¼Œé‡å‘½åä¹¦ç±æ–‡ä»¶ï¼Œå¹¶æ•´ç†åˆ°ç‹¬ç«‹æ–‡ä»¶å¤¹"""
    # ç¡®ä¿booksç›®å½•å­˜åœ¨
    if not os.path.exists(BOOKS_DIR):
        os.makedirs(BOOKS_DIR)
        print_info(f"å·²åˆ›å»ºä¹¦ç±ç›®å½•: {BOOKS_DIR}")
        return  # å¦‚æœæ˜¯æ–°åˆ›å»ºçš„ç›®å½•ï¼Œé‡Œé¢æ²¡æœ‰æ–‡ä»¶ï¼Œç›´æ¥è¿”å›
    
    # è·å–æ‰€æœ‰æ–‡ä»¶
    files = [f for f in os.listdir(BOOKS_DIR) if os.path.isfile(os.path.join(BOOKS_DIR, f))]
    print_info(f"æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶å¾…å¤„ç†")
        
    for filename in files:
        file_path = os.path.join(BOOKS_DIR, filename)
        print_info(f"\nå¼€å§‹å¤„ç†æ–‡ä»¶: {filename}")
        
        # è§£ææ–‡ä»¶å
        author, title, year, ext = parse_filename(filename)
        print_info(f"æ–‡ä»¶åè§£æç»“æœ: ä½œè€…='{author}', æ ‡é¢˜='{title}', å¹´ä»½='{year}', æ ¼å¼='{ext}'")

        # å¦‚æœæ— æ³•ä»æ–‡ä»¶åè§£æï¼Œå°è¯•ä»å…ƒæ•°æ®è·å–
        if not title or not author:
            print_info("å°è¯•ä»æ–‡ä»¶å…ƒæ•°æ®è·å–ä¿¡æ¯")
            if ext.lower() == "pdf":
                meta_author, meta_title = extract_pdf_metadata(file_path)
                if not author and meta_author: 
                    author = meta_author
                    print_info(f"ä»PDFå…ƒæ•°æ®è·å–ä½œè€…: {author}")
                if not title and meta_title: 
                    title = meta_title
                    print_info(f"ä»PDFå…ƒæ•°æ®è·å–æ ‡é¢˜: {title}")
            elif ext.lower() in ["epub", "mobi", "azw3","azw"]:
                meta_author, meta_title = extract_ebook_metadata(file_path)
                if not author and meta_author: 
                    author = meta_author
                    print_info(f"ä»ç”µå­ä¹¦å…ƒæ•°æ®è·å–ä½œè€…: {author}")
                if not title and meta_title: 
                    title = meta_title
                    print_info(f"ä»ç”µå­ä¹¦å…ƒæ•°æ®è·å–æ ‡é¢˜: {title}")
        
        # ä»è±†ç“£è·å–ä¿¡æ¯
        douban_info = None
        if title:
            print_info(f"å°è¯•ä»è±†ç“£è·å–ä¿¡æ¯: {title}")
            # ä¼ é€’é¢„æœŸçš„ä½œè€…ä¿¡æ¯
            douban_info = search_douban(title, expected_author=author)
            if douban_info:
                print_info(f"æˆåŠŸè·å–è±†ç“£ä¿¡æ¯: {douban_info['title']}")
                # ä¼˜å…ˆä½¿ç”¨è±†ç“£çš„ä½œè€…ä¿¡æ¯
                title = douban_info["title"]
                year = douban_info.get("year")
                
                # ä¼˜å…ˆä½¿ç”¨è±†ç“£è¯¦æƒ…é¡µçš„authorså­—æ®µ
                if douban_info.get("authors") and len(douban_info["authors"]) > 0:
                    author = douban_info["authors"][0]
                    print_info(f"ä½¿ç”¨è±†ç“£è¯¦æƒ…é¡µä½œè€…: {author}")
                else:
                    author = douban_info["author"]
                    print_info(f"ä½¿ç”¨è±†ç“£æœç´¢ç»“æœä½œè€…: {author}")
                
                # æ¸…ç†ä½œè€…åä¸­çš„å›½ç±æ ‡è®°
                if author:
                    original_author = author
                    # ç§»é™¤å¦‚ã€”æ³•ã€•ã€ï¼ˆç¾ï¼‰ç­‰å›½ç±æ ‡è®°
                    author = re.sub(r'[\[ï¼ˆ\(ã€ã€”][^\]ï¼‰\)ã€‘ã€•]*[\]ï¼‰\)ã€‘ã€•]', '', author).strip()
                    # ç§»é™¤å¯èƒ½çš„å‰å¯¼å’Œå°¾éƒ¨ç‰¹æ®Šå­—ç¬¦
                    author = re.sub(r'^[^\u4e00-\u9fa5a-zA-Z0-9]+|[^\u4e00-\u9fa5a-zA-Z0-9]+$', '', author).strip()
                    if author != original_author:
                        print_info(f"æ¸…ç†ä½œè€…åä¸­çš„å›½ç±æ ‡è®°: '{original_author}' -> '{author}'")

        # å¦‚æœæ ‡é¢˜æˆ–ä½œè€…ä¸ºç©ºï¼Œè¯·æ±‚ç”¨æˆ·è¾“å…¥
        if not title:
            title = input("âš ï¸ æœªèƒ½è·å–åˆ°ä¹¦ç±æ ‡é¢˜ï¼Œè¯·æ‰‹åŠ¨è¾“å…¥: ").strip()
            print_info(f"ç”¨æˆ·è¾“å…¥æ ‡é¢˜: {title}")
        if not author:
            author = input("âš ï¸ æœªèƒ½è·å–åˆ°ä½œè€…ä¿¡æ¯ï¼Œè¯·æ‰‹åŠ¨è¾“å…¥: ").strip()
            print_info(f"ç”¨æˆ·è¾“å…¥ä½œè€…: {author}")
            
        # ç¡®ä¿å¿…è¦ä¿¡æ¯å­˜åœ¨
        if not title or not author:
            print_error(f"æ— æ³•å¤„ç†æ–‡ä»¶ {filename}ï¼šç¼ºå°‘å¿…è¦çš„æ ‡é¢˜æˆ–ä½œè€…ä¿¡æ¯")
            continue

        # æ ¹æ®æ˜¯å¦æœ‰å¹´ä»½ä¿¡æ¯ä½¿ç”¨ä¸åŒçš„å‘½åæ¨¡å¼
        if year:
            folder_name = NEW_NAME_PATTERN.format(author=author, title=title, year=year)
        else:
            folder_name = f"{author} - {title}"
        
        print_info(f"ç”Ÿæˆæ–‡ä»¶å¤¹å: {folder_name}")
            
        folder_path = os.path.join(BOOKS_DIR, folder_name)
        new_book_path = os.path.join(folder_path, f"{title}.{ext}")

        # æ˜¾ç¤ºå°†è¦æ‰§è¡Œçš„æ“ä½œå¹¶ç­‰å¾…ç”¨æˆ·ç¡®è®¤
        print("\n" + colorama.Fore.CYAN + "=== å³å°†æ‰§è¡Œä»¥ä¸‹æ“ä½œ ===" + colorama.Style.RESET_ALL)
        print(f"{colorama.Fore.WHITE}åŸæ–‡ä»¶: {colorama.Fore.YELLOW}{filename}{colorama.Style.RESET_ALL}")
        print(f"{colorama.Fore.WHITE}æ–°æ–‡ä»¶å¤¹: {colorama.Fore.GREEN}{folder_name}{colorama.Style.RESET_ALL}")
        print(f"{colorama.Fore.WHITE}æ–°æ–‡ä»¶å: {colorama.Fore.GREEN}{title}.{ext}{colorama.Style.RESET_ALL}")
        if douban_info and douban_info.get("cover_url"):
            print(f"{colorama.Fore.WHITE}å°†ä¸‹è½½è±†ç“£å°é¢{colorama.Style.RESET_ALL}")
        print(f"{colorama.Fore.WHITE}å°†ç”ŸæˆNFOæ–‡ä»¶{colorama.Style.RESET_ALL}")
        
        confirm = input(f"\n{colorama.Fore.YELLOW}æ˜¯å¦ç»§ç»­ï¼Ÿ(è¾“å…¥ 'no' å–æ¶ˆï¼Œå…¶ä»–ä»»æ„é”®ç»§ç»­): {colorama.Style.RESET_ALL}").strip().lower()
        if confirm == 'no':
            print_info("ç”¨æˆ·å–æ¶ˆæ“ä½œ")
            print(f"{colorama.Fore.RED}å·²å–æ¶ˆæ“ä½œ{colorama.Style.RESET_ALL}")
            continue

        # æ‰§è¡Œæ–‡ä»¶æ“ä½œ
        try:
            os.makedirs(folder_path, exist_ok=True)
            print_info(f"åˆ›å»ºæ–‡ä»¶å¤¹: {folder_path}")
            
            os.rename(file_path, new_book_path)
            print_info(f"é‡å‘½åæ–‡ä»¶: {file_path} -> {new_book_path}")

            if douban_info and douban_info.get("cover_url"):
                cover_path = os.path.join(folder_path, f"{title}.jpg")
                download_cover(douban_info["cover_url"], cover_path)

            nfo_path = os.path.join(folder_path, f"{title}.nfo")
            generate_nfo(douban_info, nfo_path)
            print_info(f"ç”ŸæˆNFOæ–‡ä»¶: {nfo_path}")
            
            print_info(f"æ–‡ä»¶å¤„ç†å®Œæˆ: {title}")
            print(f"{colorama.Fore.GREEN}âœ… æ–‡ä»¶å¤„ç†å®Œæˆ: {title}\n{colorama.Style.RESET_ALL}")
        except Exception as e:
            print_error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            print(f"{colorama.Fore.RED}âŒ å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {e}{colorama.Style.RESET_ALL}")


# === é…ç½®ç®¡ç† ===
def save_config():
    """ä¿å­˜å½“å‰é…ç½®åˆ°æ–‡ä»¶"""
    config = {
        'proxy': REQUEST_CONFIG['proxy'],
        'request_delay': REQUEST_CONFIG['request_delay'],
        'retry_delay': REQUEST_CONFIG['retry_delay'],
        'timeout': REQUEST_CONFIG['timeout'],
        'max_retries': REQUEST_CONFIG['max_retries']
    }
    try:
        with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        print_info(f"é…ç½®å·²ä¿å­˜åˆ° {CONFIG_FILE}")
    except Exception as e:
        print_error(f"ä¿å­˜é…ç½®å¤±è´¥: {e}")

def load_config():
    """ä»æ–‡ä»¶åŠ è½½é…ç½®"""
    if not os.path.exists(CONFIG_FILE):
        print_info("æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶")
        return False
        
    try:
        with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
            config = json.load(f)
            
        # æ›´æ–°é…ç½®
        for key, value in config.items():
            if key in REQUEST_CONFIG:
                REQUEST_CONFIG[key] = value
                
        print_info(f"å·²åŠ è½½é…ç½®: {CONFIG_FILE}")
        
        # æ˜¾ç¤ºå½“å‰é…ç½®
        if REQUEST_CONFIG['proxy']:
            proxy_info = REQUEST_CONFIG['proxy'].get('http', 'None')
            print_info(f"  - ä»£ç†: {proxy_info}")
        print_info(f"  - è¯·æ±‚å»¶è¿Ÿ: {REQUEST_CONFIG['request_delay'][0]}-{REQUEST_CONFIG['request_delay'][1]}ç§’")
        print_info(f"  - è¶…æ—¶: {REQUEST_CONFIG['timeout']}ç§’")
        print_info(f"  - æœ€å¤§é‡è¯•: {REQUEST_CONFIG['max_retries']}æ¬¡")
        
        return True
    except Exception as e:
        print_error(f"åŠ è½½é…ç½®å¤±è´¥: {e}")
        return False


if __name__ == "__main__":
    # æ˜¾ç¤ºå­—ç¬¦ç”»å’Œä½œè€…ä¿¡æ¯
    print(colorama.Fore.BLUE + ASCII_ART + colorama.Style.RESET_ALL)
    print(colorama.Fore.YELLOW + AUTHOR_INFO + colorama.Style.RESET_ALL)
    
    print_info("ç”µå­ä¹¦æ–‡ä»¶æ•´ç†å·¥å…·å¯åŠ¨")
    
    print(colorama.Fore.CYAN + "=" * 50 + colorama.Style.RESET_ALL)
    print(colorama.Fore.MAGENTA + "ğŸ“š ç”µå­ä¹¦æ–‡ä»¶æ•´ç†å·¥å…·" + colorama.Style.RESET_ALL)
    print(colorama.Fore.CYAN + "=" * 50 + colorama.Style.RESET_ALL)
    print(colorama.Fore.WHITE + "åŠŸèƒ½ï¼šè‡ªåŠ¨è§£æç”µå­ä¹¦æ–‡ä»¶åï¼Œæ•´ç†åˆ°ç‹¬ç«‹æ–‡ä»¶å¤¹ï¼Œå¹¶è·å–è±†ç“£ä¿¡æ¯" + colorama.Style.RESET_ALL)
    print(colorama.Fore.WHITE + f"ä¹¦ç±ç›®å½•: {colorama.Fore.GREEN}{BOOKS_DIR}{colorama.Style.RESET_ALL}")
    print(colorama.Fore.WHITE + "æ”¯æŒæ ¼å¼: " + colorama.Fore.GREEN + ", ".join(SUPPORTED_FORMATS) + colorama.Style.RESET_ALL)
    print(colorama.Fore.CYAN + "=" * 50 + colorama.Style.RESET_ALL)
    
    # åŠ è½½é…ç½®
    has_config = load_config()
    
    # é…ç½®ç½‘ç»œè®¾ç½®
    print("\n" + colorama.Fore.CYAN + "=== ç½‘ç»œè®¾ç½® ===" + colorama.Style.RESET_ALL)
    if has_config:
        use_saved = input(f"{colorama.Fore.YELLOW}æ£€æµ‹åˆ°å·²ä¿å­˜çš„é…ç½®ï¼Œæ˜¯å¦ä½¿ç”¨? (y/n, é»˜è®¤: y): {colorama.Style.RESET_ALL}").strip().lower() != 'n'
        if not use_saved:
            print_info("ç”¨æˆ·é€‰æ‹©ä¸ä½¿ç”¨å·²ä¿å­˜çš„é…ç½®")
            has_config = False
    
    if not has_config:
        print_info("å¼€å§‹é…ç½®ç½‘ç»œè®¾ç½®")
        print(f"{colorama.Fore.WHITE}(å¯é€‰) é…ç½®ä»£ç†å’Œè¯·æ±‚å‚æ•°ï¼Œé¿å…IPå°é”{colorama.Style.RESET_ALL}")
        
        use_proxy = input(f"{colorama.Fore.YELLOW}æ˜¯å¦ä½¿ç”¨ä»£ç†? (y/n, é»˜è®¤: n): {colorama.Style.RESET_ALL}").strip().lower() == 'y'
        if use_proxy:
            proxy_host = input(f"{colorama.Fore.YELLOW}è¯·è¾“å…¥ä»£ç†åœ°å€ (ä¾‹å¦‚: 127.0.0.1): {colorama.Style.RESET_ALL}").strip()
            proxy_port = input(f"{colorama.Fore.YELLOW}è¯·è¾“å…¥ä»£ç†ç«¯å£ (ä¾‹å¦‚: 7890): {colorama.Style.RESET_ALL}").strip()
            if proxy_host and proxy_port:
                proxy_url = f"http://{proxy_host}:{proxy_port}"
                REQUEST_CONFIG['proxy'] = {
                    'http': proxy_url,
                    'https': proxy_url
                }
                print_info(f"è®¾ç½®ä»£ç†: {proxy_url}")
                print(f"{colorama.Fore.GREEN}âœ… å·²è®¾ç½®ä»£ç†: {proxy_url}{colorama.Style.RESET_ALL}")
        
        # é…ç½®è¯·æ±‚å»¶è¿Ÿ
        custom_delay = input(f"{colorama.Fore.YELLOW}æ˜¯å¦è‡ªå®šä¹‰è¯·æ±‚å»¶è¿Ÿ? (y/n, é»˜è®¤: n): {colorama.Style.RESET_ALL}").strip().lower() == 'y'
        if custom_delay:
            try:
                min_delay = float(input(f"{colorama.Fore.YELLOW}æœ€å°å»¶è¿Ÿç§’æ•° (é»˜è®¤: 1): {colorama.Style.RESET_ALL}").strip() or "1")
                max_delay = float(input(f"{colorama.Fore.YELLOW}æœ€å¤§å»¶è¿Ÿç§’æ•° (é»˜è®¤: 3): {colorama.Style.RESET_ALL}").strip() or "3")
                if min_delay > 0 and max_delay >= min_delay:
                    REQUEST_CONFIG['request_delay'] = [min_delay, max_delay]
                    print_info(f"è®¾ç½®è¯·æ±‚å»¶è¿Ÿ: {min_delay}-{max_delay}ç§’")
                    print(f"{colorama.Fore.GREEN}âœ… å·²è®¾ç½®è¯·æ±‚å»¶è¿Ÿ: {min_delay}-{max_delay}ç§’{colorama.Style.RESET_ALL}")
            except ValueError:
                print_warning("è¾“å…¥æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å»¶è¿Ÿè®¾ç½®")
                print(f"{colorama.Fore.RED}âš ï¸ è¾“å…¥æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å»¶è¿Ÿè®¾ç½®{colorama.Style.RESET_ALL}")
        
        # ä¿å­˜é…ç½®
        save_config_choice = input(f"{colorama.Fore.YELLOW}æ˜¯å¦ä¿å­˜å½“å‰é…ç½®? (y/n, é»˜è®¤: y): {colorama.Style.RESET_ALL}").strip().lower() != 'n'
        if save_config_choice:
            save_config()
    
    print_info("å¼€å§‹å¤„ç†ä¹¦ç±æ–‡ä»¶")
    print(f"\n{colorama.Fore.CYAN}å¼€å§‹å¤„ç†ä¹¦ç±æ–‡ä»¶...{colorama.Style.RESET_ALL}")
    rename_books()
    
    print_info("å¤„ç†å®Œæˆ")
    print(f"\n{colorama.Fore.GREEN}âœ… å¤„ç†å®Œæˆï¼{colorama.Style.RESET_ALL}")
    print(colorama.Fore.CYAN + "=" * 50 + colorama.Style.RESET_ALL)
